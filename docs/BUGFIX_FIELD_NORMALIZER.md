# üêõ Bugfix: Field Normalizer Service - Hardcodierte URL

**Datum:** 19. Okt 2025  
**Status:** ‚úÖ GEFIXT (Update: Proxy + Label-Qualit√§t)

---

## üêõ Update: Proxy 500 Error + Unsaubere Labels

**Datum:** 19. Okt 2025 (12:45 Uhr)

### Zus√§tzliche Probleme gefunden

1. **500 Internal Server Error vom Proxy**
   - FieldNormalizerService sendete kein `model` Parameter
   - Proxy ben√∂tigt `model` f√ºr API-Calls

2. **Unsaubere Label-Output von KI**
   ```
   ‚ùå "P√§dagogik (auch: Erziehungswissenschaften)"
   ‚ùå "Open Educational Resources (auch: OER, OER)"
   ‚ùå "Politik (auch: Politische Bildung)"
   
   ‚úÖ Sollte sein: "P√§dagogik", "Open Educational Resources", "Politik"
   ```

### Fixes implementiert

#### Fix 1: Model-Parameter hinzugef√ºgt

```typescript
// Get model from environment
const provider = environment.llmProvider || 'b-api-openai';
const providerConfig = (environment as any)[this.getProviderConfigKey(provider)];
const model = providerConfig?.model || 'gpt-4.1-mini';

return this.http.post<any>(this.apiUrl, {
  model: model,  // ‚úÖ Jetzt vorhanden!
  messages: [...]
});
```

#### Fix 2: System-Prompt versch√§rft

```typescript
// VORHER
{ role: 'system', content: 'You are a data normalization assistant. Return only the normalized value without explanation.' }

// NACHHER (versch√§rft)
{ role: 'system', content: 'You are a data normalization assistant. Return ONLY the normalized value without any explanation, parentheses, or additional text.' }
```

#### Fix 3: Labels in Vocabulary bereinigt

```typescript
// Entfernt "(auch: ...)" aus Labels bevor sie an KI geschickt werden
const cleanLabel = concept.label.replace(/\s*\(auch:.*?\)/gi, '').trim();
prompt += `${idx + 1}. "${cleanLabel}"`;  // ‚úÖ Sauber!
```

#### Fix 4: Explizite Prompt-Anweisungen

```
**KRITISCH - Ausgabe-Format:**
- Nur das reine Label zur√ºckgeben
- KEINE Klammern wie "(auch: ...)"
- KEINE zus√§tzlichen Erkl√§rungen
- Exakte Gro√ü-/Kleinschreibung aus der Liste

**Beispiele:**
- Eingabe: "P√§dagogik (auch: xyz)" ‚Üí Ausgabe: "P√§dagogik"
- Eingabe: "Politik oder so" ‚Üí Ausgabe: "Politik"
```

---

**Status:** ‚úÖ ALLE FIXES IMPLEMENTIERT

---

## üîç Problem

### Symptom

```
POST http://localhost:8000/generate net::ERR_CONNECTION_REFUSED
‚ùå Normalization failed for ccm:wwwurl
```

**Browser Console Log:**
- Viele Fehler: `POST http://localhost:8000/generate net::ERR_CONNECTION_REFUSED`
- Betraf: Field Normalization w√§hrend der Extraktion
- Service konnte nicht erreicht werden

### Root Cause

`src/app/services/field-normalizer.service.ts` hatte eine **hardcodierte URL**:

```typescript
// ‚ùå VORHER (falsch)
export class FieldNormalizerService {
  private apiUrl = 'http://localhost:8000';  // ‚Üê Hardcoded!
  
  constructor(private http: HttpClient) {}
}
```

**Probleme:**
1. ‚ùå Port 8000 existiert nicht (sollte 3001 sein f√ºr lokalen Proxy)
2. ‚ùå Keine Unterscheidung zwischen lokal/production
3. ‚ùå Nutzt nicht die environment-basierte Konfiguration
4. ‚ùå Legacy API-Format (`/generate` endpoint mit `prompt` param)

---

## ‚úÖ L√∂sung

### 1. Environment-basierte Proxy-URL

```typescript
// ‚úÖ NACHHER (korrekt)
import { environment } from '../../environments/environment';

export class FieldNormalizerService {
  private apiUrl: string;  // ‚Üê Dynamisch

  constructor(private http: HttpClient) {
    const provider = environment.llmProvider || 'b-api-openai';
    const providerConfig = (environment as any)[this.getProviderConfigKey(provider)];
    
    if (environment.production) {
      // Production: Use Netlify Function
      this.apiUrl = providerConfig?.proxyUrl || '/.netlify/functions/openai-proxy';
    } else {
      // Local: Use local proxy
      this.apiUrl = providerConfig?.proxyUrl || 'http://localhost:3001/llm';
    }
    
    console.log('üîß FieldNormalizerService initialized with proxy:', this.apiUrl);
  }
}
```

### 2. OpenAI-kompatibles API-Format

```typescript
// ‚ùå VORHER (Legacy Format)
return this.http.post<any>(`${this.apiUrl}/generate`, {
  prompt: prompt,
  temperature: 0.1,
  max_tokens: 200
});

// ‚úÖ NACHHER (OpenAI Format)
return this.http.post<any>(this.apiUrl, {
  messages: [
    { role: 'system', content: 'You are a data normalization assistant.' },
    { role: 'user', content: prompt }
  ],
  temperature: 0.1,
  max_tokens: 200
});
```

### 3. Response-Parsing angepasst

```typescript
// ‚úÖ Unterst√ºtzt beide Formate (OpenAI & Legacy)
map(response => {
  const content = response.choices?.[0]?.message?.content || response.content || '';
  return this.parseNormalizationResponse(content, field, userInput);
})
```

---

## üîÑ Neue Architektur

### Lokal (Development)

```
FieldNormalizerService
  ‚Üì
http://localhost:3001/llm
  ‚Üì
local-universal-proxy.js
  ‚Üì
OpenAI/B-API
```

### Production (Netlify)

```
FieldNormalizerService
  ‚Üì
/.netlify/functions/openai-proxy
  ‚Üì
Netlify Function
  ‚Üì
OpenAI/B-API
```

---

## ‚úÖ Vorteile der L√∂sung

| Aspekt | Vorher | Nachher |
|--------|--------|---------|
| **URL** | Hardcoded `localhost:8000` | Environment-basiert |
| **Lokal** | ‚ùå Connection Refused | ‚úÖ Nutzt Proxy (Port 3001) |
| **Production** | ‚ùå W√ºrde nicht funktionieren | ‚úÖ Nutzt Netlify Functions |
| **API-Format** | Legacy (`/generate`, `prompt`) | OpenAI-kompatibel (`messages`) |
| **Provider** | Fixed | Folgt `environment.llmProvider` |
| **Wartbarkeit** | Schwierig | Zentrale Konfiguration |

---

## üß™ Testing

### Test 1: Lokaler Modus

```bash
# Terminal 1: Proxy starten
npm run proxy

# Terminal 2: App starten
npm start

# Test: Extraktion starten
# ‚úÖ Keine Connection Refused Errors mehr
# ‚úÖ Normalization l√§uft √ºber localhost:3001/llm
```

**Erwartete Logs:**
```
üîß FieldNormalizerService initialized with proxy: http://localhost:3001/llm
üìù Normalization prompt for cclom:title: ...
üì• Raw response for cclom:title: {...}
‚úÖ Normalized cclom:title: "..." ‚Üí "..."
```

### Test 2: Production Build

```bash
npm run build
# ‚úÖ apiUrl wird auf /.netlify/functions/openai-proxy gesetzt
```

---

## üìä Impact

### Services betroffen

- ‚úÖ `FieldNormalizerService` - GEFIXT

### Services die korrekt waren

- ‚úÖ `OpenAIProxyService` - Nutzt bereits environment.ts
- ‚úÖ `GeocodingService` - Nutzt bereits environment.ts
- ‚úÖ `GuestSubmissionService` - Nutzt bereits environment.ts

---

## üîç Wie wurde das Problem gefunden?

1. **User Report:** Browser Console zeigte Errors
2. **Log-Analyse:** `POST http://localhost:8000/generate net::ERR_CONNECTION_REFUSED`
3. **Code-Suche:** `grep -r "8000" src/` ‚Üí Fand hardcodierte URL
4. **Root Cause:** Service nutzte nicht die gleiche Proxy-Architektur wie andere Services

---

## üìö Lessons Learned

### ‚ùå Nicht mehr machen

- Hardcodierte URLs in Services
- Legacy API-Formate nutzen
- Direkte API-Calls ohne Proxy

### ‚úÖ Best Practices

- Immer `environment.ts` f√ºr Konfiguration nutzen
- Konsistente Proxy-Architektur √ºber alle Services
- OpenAI-kompatibles Format f√ºr LLM-Calls
- Environment-abh√§ngige URLs (lokal vs production)

---

## üéØ Zusammenfassung

**Problem:** FieldNormalizerService hatte hardcodierte URL `localhost:8000/generate`

**L√∂sung:** 
1. ‚úÖ Environment-basierte Proxy-URL (wie andere Services)
2. ‚úÖ OpenAI-kompatibles API-Format
3. ‚úÖ Lokal: `http://localhost:3001/llm`
4. ‚úÖ Production: `/.netlify/functions/openai-proxy`

**Status:** ‚úÖ **KOMPLETT GEFIXT**

**Testing:** Nach `npm start` keine Connection Refused Errors mehr!
