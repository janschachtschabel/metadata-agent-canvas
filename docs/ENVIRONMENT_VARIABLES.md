# Environment Variables Guide

Das `replace-env.js` Script unterst√ºtzt jetzt **drei LLM-Provider** und kann deren Konfiguration aus Environment Variables injizieren.

---

## üìã Unterst√ºtzte Environment Variables

### **LLM Provider Selection**

| Variable | Beschreibung | Beispiel |
|----------|-------------|----------|
| `LLM_PROVIDER` | Welcher Provider verwendet werden soll | `openai`, `b-api-openai`, oder `b-api-academiccloud` |

---

### **OpenAI Configuration**

| Variable | Beschreibung | Standard | Beispiel |
|----------|-------------|----------|----------|
| `OPENAI_API_KEY` | OpenAI API Key | - | `sk-proj-abc123...` |
| `OPENAI_MODEL` | Modell-Name | `gpt-4.1-mini` | `gpt-4o-mini` |
| `OPENAI_BASE_URL` | Custom Base URL | _(leer)_ | `https://api.openai.com/v1` |
| `GPT5_REASONING_EFFORT` | GPT-5 Reasoning Level | `medium` | `low`, `medium`, `high` |
| `GPT5_VERBOSITY` | GPT-5 Verbosity | `low` | `low`, `medium`, `high` |

---

### **B-API Configuration**

**Beide B-API Provider (`b-api-openai` und `b-api-academiccloud`) verwenden den gleichen API Key:**

| Variable | Beschreibung | Standard | Beispiel |
|----------|-------------|----------|----------|
| `B_API_KEY` | B-API Key (f√ºr beide B-API Provider) | - | `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx` |
| `B_MODEL` | Modell-Name (optional) | Provider-abh√§ngig | `gpt-4.1-mini` |

**Provider-spezifische Defaults:**
- **b-api-openai:** `gpt-4.1-mini` (OpenAI-kompatibel)
- **b-api-academiccloud:** `deepseek-r1` (AcademicCloud)

---

## ü™ü Windows - Environment Variables setzen

### **PowerShell:**

```powershell
# LLM Provider ausw√§hlen
$env:LLM_PROVIDER="b-api-openai"  # oder "b-api-academiccloud" oder "openai"

# OpenAI
$env:OPENAI_API_KEY="sk-proj-..."
$env:OPENAI_MODEL="gpt-4.1-mini"

# B-API (f√ºr beide B-API Provider)
$env:B_API_KEY="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
$env:B_MODEL="gpt-4.1-mini"  # Optional

# App starten
npm start
```

### **CMD:**

```cmd
set LLM_PROVIDER=b-api-openai
set OPENAI_API_KEY=sk-proj-...
set B_API_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
npm start
```

---

## üêß Linux/Mac - Environment Variables setzen

### **Bash/Zsh:**

```bash
# LLM Provider ausw√§hlen
export LLM_PROVIDER="b-api-openai"  # oder "b-api-academiccloud" oder "openai"

# OpenAI
export OPENAI_API_KEY="sk-proj-..."
export OPENAI_MODEL="gpt-4.1-mini"

# B-API (f√ºr beide B-API Provider)
export B_API_KEY="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
export B_MODEL="gpt-4.1-mini"  # Optional

# App starten
npm start
```

---

## ‚òÅÔ∏è Netlify - Environment Variables konfigurieren

**Netlify Dashboard ‚Üí Site Settings ‚Üí Environment Variables:**

### **Alle Provider parallel:**

```
LLM_PROVIDER=b-api-openai
OPENAI_API_KEY=sk-proj-...
B_API_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

**Provider wechseln:** √Ñndern Sie einfach `LLM_PROVIDER` zu:
- `openai` ‚Üí OpenAI direkt
- `b-api-openai` ‚Üí B-API mit OpenAI-kompatiblen Modellen
- `b-api-academiccloud` ‚Üí B-API mit AcademicCloud (deepseek-r1)

**Wichtig:** Bei Netlify wird `LLM_PROVIDER` nur verwendet, wenn es in `environment.prod.ts` als leer konfiguriert ist.

---

## üîÑ Verhalten des replace-env.js Scripts

### **1. Start-Output**

Beim Ausf√ºhren von `npm start` oder `npm run build` sehen Sie:

```
üîß Processing environment files...
üìã Environment variables:

üîπ LLM Provider:
  - LLM_PROVIDER: b-api-openai

üîπ OpenAI Configuration:
  - OPENAI_API_KEY: ‚úÖ Found
  - OPENAI_MODEL: gpt-4.1-mini
  - OPENAI_BASE_URL: (empty)
  - GPT5_REASONING_EFFORT: medium
  - GPT5_VERBOSITY: low

üîπ B-API Configuration:
  - B_API_KEY: ‚úÖ Found
  - B_MODEL: (using provider default)
  - BASE_URLs:
    ‚Ä¢ b-api-openai: https://b-api.staging.openeduhub.net/api/v1/llm/openai
    ‚Ä¢ b-api-academiccloud: https://b-api.staging.openeduhub.net/api/v1/llm/academiccloud

üìù Processing environment.ts...
  ‚ÑπÔ∏è  File already contains an API key, skipping injection
  
üìù Processing environment.prod.ts...
  ‚úÖ Injected LLM_PROVIDER: b-api-openai
  ‚úÖ Injected B_API_KEY for bApiOpenai
  ‚úÖ environment.prod.ts updated

‚úÖ Environment processing complete
```

---

### **2. Injection-Regeln**

| Bedingung | Verhalten |
|-----------|-----------|
| **API Key in Datei vorhanden** | ‚ùå Keine Injection (Datei-Wert wird bevorzugt) |
| **API Key in Datei leer** | ‚úÖ Injection aus Environment Variable |
| **Keine Environment Variable** | ‚ÑπÔ∏è Datei-Wert wird beibehalten |

**Beispiel:**

```typescript
// environment.ts
openai: {
  apiKey: 'sk-proj-...',  // ‚Üê Vorhandener Key
}
bApiOpenai: {
  apiKey: 'xxxxxxxx-xxxx-...',  // ‚Üê Vorhandener Key
}
```

**Script-Verhalten:**
```
‚ÑπÔ∏è  File already contains an API key, skipping injection
üí° To inject from environment variables, remove the existing key first
```

**L√∂sung:**
```typescript
// environment.ts
openai: {
  apiKey: '',  // ‚Üê Leerer Key = Injection aktiviert
}
bApiOpenai: {
  apiKey: '',  // ‚Üê Leerer Key = Injection aktiviert
}
```

---

## üéØ Use Cases

### **Use Case 1: Nur B-API OpenAI lokal**

```powershell
# Nur B-API Key setzen
$env:B_API_KEY="xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"
$env:LLM_PROVIDER="b-api-openai"

# environment.ts:
llmProvider: 'b-api-openai',
bApiOpenai: {
  apiKey: '',  # ‚Üê Leer f√ºr Injection
}

npm start
```

**Ergebnis:**
```
‚úÖ Injected LLM_PROVIDER: b-api-openai
‚úÖ Injected B_API_KEY for bApiOpenai
```

---

### **Use Case 2: Alle Provider parallel**

```powershell
$env:OPENAI_API_KEY="sk-proj-..."
$env:B_API_KEY="xxxxxxxx-xxxx-..."
$env:LLM_PROVIDER="b-api-openai"  # Aktiver Provider

npm start
```

**Wechsel zur Laufzeit:**
```typescript
// In environment.ts √§ndern:
llmProvider: 'openai',              // ‚Üê OpenAI
llmProvider: 'b-api-openai',        // ‚Üê B-API OpenAI-kompatibel
llmProvider: 'b-api-academiccloud', // ‚Üê B-API AcademicCloud
```

---

### **Use Case 3: Netlify Production**

**Netlify Dashboard:**
```
LLM_PROVIDER=b-api-openai
B_API_KEY=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
```

**environment.prod.ts:**
```typescript
llmProvider: '',  // ‚Üê Leer f√ºr Injection
bApiOpenai: {
  apiKey: '',     // ‚Üê Leer f√ºr Injection
}
bApiAcademicCloud: {
  apiKey: '',     // ‚Üê Leer f√ºr Injection (gleicher B_API_KEY)
}
```

**Build-Befehl:** `npm run build`

**Script injiziert automatisch:**
```typescript
llmProvider: 'b-api-openai',
bApiOpenai: {
  apiKey: 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx',
}
bApiAcademicCloud: {
  apiKey: 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx',
}
```

---

## ‚ö†Ô∏è Wichtige Hinweise

### **Sicherheit**

- ‚ùå **NIEMALS** API Keys in Git committen
- ‚úÖ Verwenden Sie Environment Variables f√ºr Production
- ‚úÖ F√ºr lokale Entwicklung: Keys in `.env` (nicht in Git!)

### **Priorit√§t**

1. **Datei-Wert** (wenn vorhanden)
2. **Environment Variable** (wenn Datei leer)
3. **Standard-Wert** (wenn beides fehlt)

### **Netlify Deploy**

Das Script l√§uft automatisch bei:
- `npm start` (lokal)
- `npm run build` (Netlify)

Netlify nutzt die Environment Variables aus dem Dashboard.

---

## üß™ Testing

### **Pr√ºfen welcher Provider aktiv ist:**

```bash
npm start
```

**Console pr√ºfen:**
```
üîß Development mode: Using local proxy for b-api-openai
üåê Proxy URL: http://localhost:3001
```

**Oder f√ºr AcademicCloud:**
```
üîß Development mode: Using local proxy for b-api-academiccloud
üåê Proxy URL: http://localhost:3001
```

### **Environment Variable pr√ºfen:**

**PowerShell:**
```powershell
echo $env:B_API_KEY
echo $env:LLM_PROVIDER
```

**Linux/Mac:**
```bash
echo $B_API_KEY
echo $LLM_PROVIDER
```

---

## üìö Weitere Dokumentation

- **Multi-Provider Setup:** Siehe `LLM_PROVIDER_CONFIGURATION.md`
- **Netlify Deployment:** Siehe `README.md`
- **API Unterschiede:** Siehe `LLM_PROVIDER_CONFIGURATION.md`

---

**Stand:** 15. Oktober 2025
