# ===========================
# Deployment Platform
# ===========================

# Deployment Platform
# Controls which API endpoints are used at BUILD TIME
# 
# ‚ö†Ô∏è PRIORITY (highest first):
# 1. Environment Variable (Vercel/Netlify/CI) - HIGHEST PRIORITY
# 2. This .env file (local development) - MEDIUM PRIORITY
# 3. Hardcoded in environment.prod.ts - FALLBACK ONLY
#
# Options: vercel, netlify, local, auto
# - vercel: Uses /api/* endpoints (for Vercel deployment)
# - netlify: Uses /.netlify/functions/* endpoints (for Netlify deployment)
# - local: Uses http://localhost:3001/* (for local development)
# - auto: Auto-detect based on hostname (fallback)
#
# üí° For local development:
DEPLOYMENT_PLATFORM=local
#
# üí° For local testing of Vercel config:
# DEPLOYMENT_PLATFORM=vercel
#
# üí° For local testing of Netlify config:
# DEPLOYMENT_PLATFORM=netlify

# ===========================
# LLM Provider Configuration
# ===========================

# LLM Provider Selection (Optional)
# Options: openai, b-api-openai, b-api-academiccloud
# Default: b-api-openai
# - openai: Direct OpenAI API (requires OPENAI_API_KEY)
# - b-api-openai: B-API OpenAI-compatible endpoint (requires B_API_KEY)
# - b-api-academiccloud: B-API AcademicCloud with DeepSeek-R1 (requires B_API_KEY)
LLM_PROVIDER=b-api-openai

# OpenAI API Key (get from: https://platform.openai.com/api-keys)
# Only needed if LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-api-key-here

# B-API Key (for B-API OpenAI and AcademicCloud providers)
# Needed if LLM_PROVIDER=b-api-openai or b-api-academiccloud
B_API_KEY=your-uuid-key-here

# OpenAI Model
# Options: gpt-5-mini, gpt-5-nano, gpt-5, gpt-4.1-mini, gpt-4o, gpt-4o-mini, gpt-4-turbo, etc.
OPENAI_MODEL=gpt-4.1-mini

# OpenAI Base URL (optional, leave empty for default)
OPENAI_BASE_URL=

# ===========================
# GPT-5 Performance Settings
# ===========================
# ‚ö†Ô∏è NOTE: These parameters are ONLY used when OPENAI_MODEL starts with "gpt-5"
# For other models (gpt-4, gpt-3.5, etc.), these settings are ignored.

# Reasoning Effort (Optional - GPT-5 only)
# Controls how much "thinking time" GPT-5 invests
# Default: minimal
# Options: minimal, low, medium, high
# - minimal: Fast extraction (~0.5-1s per call) - Recommended for structured data
# - low: Better quality (~1-2s per call)
# - medium: High quality (~2-3s per call)
# - high: Maximum quality (~3-5s per call)
GPT5_REASONING_EFFORT=minimal

# Response Verbosity (Optional - GPT-5 only)
# Controls the length of responses
# Default: low
# Options: low, medium, high
# - low: Concise, minimal tokens - Recommended for extraction
# - medium: Balanced responses
# - high: Detailed, verbose responses
GPT5_VERBOSITY=low
